{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d6e5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==1.0.0\n",
      "  Downloading langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: chromadb in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain==1.0.0) (1.0.6)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain==1.0.0) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain==1.0.0) (2.11.7)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (3.11.4)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (0.4.43)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (3.5.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.30.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.0.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.0.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.0.0) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.29.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (2.1)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain==1.0.0) (1.12.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain==1.0.0) (0.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Downloading langchain-1.0.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: langchain\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 1.0.8\n",
      "    Uninstalling langchain-1.0.8:\n",
      "      Successfully uninstalled langchain-1.0.8\n",
      "Successfully installed langchain-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain==1.0.0 chromadb --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81e43d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: chromadb in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (1.0.6)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (3.11.4)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.43)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.5.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.30.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.29.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (2.1)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301a49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from operator import add as add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ====================================\n",
    "# 1. LLM and Embeddings Setup\n",
    "# ====================================\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "# ====================================\n",
    "# 2. Load Products Descriptions\n",
    "# ====================================\n",
    "products_file = \"products_descriptions.txt\"\n",
    "if not os.path.exists(products_file):\n",
    "    raise FileNotFoundError(f\"Products file not found: {products_file}\")\n",
    "\n",
    "loader = TextLoader(products_file, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "product_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_dir = \"./chroma_products\"\n",
    "if not os.path.exists(persist_dir):\n",
    "    os.makedirs(persist_dir)\n",
    "\n",
    "product_vectorstore = Chroma.from_documents(\n",
    "    documents=product_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name=\"products\"\n",
    ")\n",
    "product_retriever = product_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# ====================================\n",
    "# 3. Retriever Tool for Products\n",
    "# ====================================\n",
    "@tool\n",
    "def product_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches products_descriptions.txt and returns matching product details.\n",
    "    \"\"\"\n",
    "    results = product_retriever.invoke(query)\n",
    "    if not results:\n",
    "        return \"No relevant products found.\"\n",
    "    return \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "tools = [product_tool]\n",
    "tools_dict = {tool.name: tool for tool in tools}\n",
    "llm = llm.bind_tools(tools)\n",
    "\n",
    "# ====================================\n",
    "# 4. Agent State\n",
    "# ====================================\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    product_filters: dict      # store user-selected filters\n",
    "    current_question: str      # which filter to ask next\n",
    "\n",
    "questions_order = [\n",
    "    \"Skin type\",\n",
    "    \"Key ingredients\",\n",
    "    \"Benefits\",\n",
    "    \"Usage\",\n",
    "    \"Precautions\",\n",
    "    \"Price\",\n",
    "    \"Size\"\n",
    "]\n",
    "\n",
    "initial_state = {\"messages\": [], \"product_filters\": {}, \"current_question\": questions_order[0]}\n",
    "\n",
    "# ====================================\n",
    "# 5. LLM Node: Ask Next Question or Retrieve Products\n",
    "# ====================================\n",
    "system_prompt = \"You are a product assistant. Collect filters from the user step by step.\"\n",
    "\n",
    "def call_llm(state: AgentState) -> AgentState:\n",
    "    # Determine which question to ask next\n",
    "    current_q = state.get(\"current_question\")\n",
    "    \n",
    "    if current_q:\n",
    "        msg_content = f\"Please provide {current_q} (type 'end' to finish):\"\n",
    "        state[\"messages\"].append(AIMessage(content=msg_content))\n",
    "    else:\n",
    "        # All questions answered → query products\n",
    "        query_parts = [f\"{k}: {v}\" for k, v in state[\"product_filters\"].items()]\n",
    "        query = \" AND \".join(query_parts)\n",
    "        state[\"messages\"].append(AIMessage(content=f\"Retrieving products matching: {query}\", tool_calls=[\n",
    "            {\"id\": \"1\", \"name\": \"product_tool\", \"args\": {\"query\": query}}\n",
    "        ]))\n",
    "    \n",
    "    return state\n",
    "\n",
    "# ====================================\n",
    "# 6. Retriever Node: Process Tool Calls or User Input\n",
    "# ====================================\n",
    "def take_action(state: AgentState) -> AgentState:\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    \n",
    "    # Process user input for current question\n",
    "    if isinstance(last_msg, HumanMessage):\n",
    "        user_text = last_msg.content.strip()\n",
    "        if user_text.lower() == \"end\":\n",
    "            state[\"current_question\"] = None\n",
    "        else:\n",
    "            q = state.get(\"current_question\")\n",
    "            if q:\n",
    "                state[\"product_filters\"][q] = user_text\n",
    "                # Move to next question\n",
    "                next_index = questions_order.index(q) + 1\n",
    "                state[\"current_question\"] = questions_order[next_index] if next_index < len(questions_order) else None\n",
    "    \n",
    "    # Execute tool calls if any\n",
    "    if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "        results = []\n",
    "        for t in last_msg.tool_calls:\n",
    "            if t[\"name\"] not in tools_dict:\n",
    "                results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=\"Invalid tool name\"))\n",
    "                continue\n",
    "            result = tools_dict[t[\"name\"]].invoke(t[\"args\"].get(\"query\", \"\"))\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=result))\n",
    "        state[\"messages\"].extend(results)\n",
    "    \n",
    "    return state\n",
    "\n",
    "# ====================================\n",
    "# 7. LangGraph Workflow\n",
    "# ====================================\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"llm\", call_llm)\n",
    "graph.add_node(\"retriever_agent\", take_action)\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    return hasattr(last_msg, \"tool_calls\") and len(last_msg.tool_calls) > 0\n",
    "\n",
    "graph.add_conditional_edges(\"llm\", should_continue, {True: \"retriever_agent\", False: END})\n",
    "graph.add_edge(\"retriever_agent\", \"llm\")\n",
    "graph.set_entry_point(\"llm\")\n",
    "\n",
    "rag_agent = graph.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4981231",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StateGraph' object has no attribute 'to_mermaid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Export the graph to mermaid format\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m mermaid_code \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mto_mermaid()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Display it as a Mermaid diagram\u001b[39;00m\n\u001b[0;32m      7\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```mermaid\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmermaid_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StateGraph' object has no attribute 'to_mermaid'"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Export the graph to mermaid format\n",
    "mermaid_code = graph.to_mermaid()\n",
    "\n",
    "# Display it as a Mermaid diagram\n",
    "display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Your Personal Skincare Advisor\n",
      "\n",
      "Hi! I'm here to find you the perfect product. Just talk to me naturally!\n",
      "\n",
      "Advisor: Hey! Tell me about your skin or what you're looking for — I'm all ears!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence, Literal\n",
    "\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ===============================\n",
    "# LLM & Embeddings\n",
    "# ===============================\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",   # or gemini-1.5-pro / gemini-2.0-flash-exp\n",
    "    temperature=0.7,\n",
    "    convert_system_message_to_human=True   # ← THIS FIXES THE ERROR\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Load & Index Products\n",
    "# ===============================\n",
    "products_file = \"products_descriptions.txt\"\n",
    "if not os.path.exists(products_file):\n",
    "    raise FileNotFoundError(f\"Products file not found: {products_file}\")\n",
    "\n",
    "loader = TextLoader(products_file, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_products\",\n",
    "    collection_name=\"products\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# ===============================\n",
    "# Tool\n",
    "# ===============================\n",
    "@tool\n",
    "def search_products(query: str) -> str:\n",
    "    \"\"\"Search the product database.\"\"\"\n",
    "    results = retriever.invoke(query)\n",
    "    if not results:\n",
    "        return \"No products found matching your criteria.\"\n",
    "    return \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "tools = [search_products]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# ===============================\n",
    "# State\n",
    "# ===============================\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], \"Conversation history\"]\n",
    "    collected_info: dict\n",
    "    step: Literal[\"asking\", \"searching\", \"recommending\"]\n",
    "\n",
    "# ===============================\n",
    "# System Prompt\n",
    "# ===============================\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a warm, expert skincare advisor. Your job is to help the customer find the PERFECT product.\n",
    "\n",
    "Be friendly, caring, and a little playful. Speak naturally.\n",
    "\n",
    "Ask ONE smart question at a time when needed.\n",
    "\n",
    "When you have enough info (at least skin type + main concern), you may call the search_products tool.\n",
    "\n",
    "If the user says \"done\", \"end\", \"recommend\", or \"that's all\" → call the tool immediately.\n",
    "\n",
    "Already known:\n",
    "{collected_info}\n",
    "\n",
    "Respond ONLY with natural conversation.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# Nodes\n",
    "# ===============================\n",
    "def interviewer(state: AgentState) -> dict:\n",
    "    collected = state.get(\"collected_info\", {})\n",
    "    info_str = \"\\n\".join([f\"- {k}: {v}\" for k, v in collected.items()]) if collected else \"Nothing yet\"\n",
    "    formatted = SYSTEM_PROMPT.format(collected_info=info_str)\n",
    "\n",
    "    # Build message list: System → previous turns → fake Human turn at the end\n",
    "    history = state[\"messages\"][-20:]  # keep context\n",
    "    messages = [SystemMessage(content=formatted)] + history\n",
    "\n",
    "    # ← Critical fix: Gemini requires the last message to be Human or empty\n",
    "    if not messages or not isinstance(messages[-1], HumanMessage):\n",
    "        messages.append(HumanMessage(content=\"Please continue the conversation.\"))\n",
    "\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    tool_calls = getattr(response, \"tool_calls\", [])\n",
    "    new_step = \"searching\" if tool_calls else \"asking\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"collected_info\": collected,\n",
    "        \"step\": new_step\n",
    "    }\n",
    "\n",
    "def extract_info(state: AgentState) -> dict:\n",
    "    last_user_msg = next((m for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), None)\n",
    "    if not last_user_msg:\n",
    "        return {\"collected_info\": state[\"collected_info\"]}\n",
    "\n",
    "    prompt = f'''\n",
    "Extract skincare preferences as JSON (only confident keys):\n",
    "\n",
    "Possible keys:\n",
    "- skin_type\n",
    "- concerns\n",
    "- preferred_ingredients\n",
    "- avoided_ingredients\n",
    "- texture_preference\n",
    "- budget\n",
    "- size\n",
    "- routine_step\n",
    "\n",
    "User said: \"{last_user_msg.content}\"\n",
    "\n",
    "Return valid JSON only.\n",
    "'''\n",
    "\n",
    "    try:\n",
    "        result = llm.invoke(prompt)\n",
    "        data = json.loads(result.content)\n",
    "        current = state.get(\"collected_info\", {}).copy()\n",
    "        current.update({k.strip(): v.strip() for k, v in data.items() if v})\n",
    "    except:\n",
    "        current = state.get(\"collected_info\", {})\n",
    "\n",
    "    return {\"collected_info\": current}\n",
    "\n",
    "def tool_node(state: AgentState) -> dict:\n",
    "    results = []\n",
    "    last_ai = state[\"messages\"][-1]\n",
    "    for tc in getattr(last_ai, \"tool_calls\", []):\n",
    "        result = search_products.invoke(tc[\"args\"])\n",
    "        results.append(ToolMessage(tool_call_id=tc[\"id\"], name=tc[\"name\"], content=result))\n",
    "    return {\"messages\": results, \"step\": \"recommending\"}\n",
    "\n",
    "def recommender(state: AgentState) -> dict:\n",
    "    collected = state.get(\"collected_info\", {})\n",
    "    search_result = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a luxury skincare consultant giving the final recommendation.\n",
    "\n",
    "Customer profile:\n",
    "{json.dumps(collected, indent=2)}\n",
    "\n",
    "Search results:\n",
    "{search_result}\n",
    "\n",
    "Recommend 1–3 best products. Explain why each is perfect.\n",
    "Be warm, enthusiastic, use light emojis.\n",
    "\n",
    "End with: \"Which one feels right for you, or want more options?\"\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=prompt),\n",
    "        HumanMessage(content=\"Give me the final recommendations now.\")\n",
    "    ])\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "# ===============================\n",
    "# Graph\n",
    "# ===============================\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"extract_info\", extract_info)\n",
    "workflow.add_node(\"interviewer\", interviewer)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"recommender\", recommender)\n",
    "\n",
    "workflow.set_entry_point(\"extract_info\")\n",
    "workflow.add_edge(\"extract_info\", \"interviewer\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"interviewer\",\n",
    "    lambda s: s[\"step\"],\n",
    "    {\"asking\": \"extract_info\", \"searching\": \"tools\"}\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"recommender\")\n",
    "workflow.add_edge(\"recommender\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# ===============================\n",
    "# Run\n",
    "# ===============================\n",
    "def run_advisor():\n",
    "    print(\"Welcome to Your Personal Skincare Advisor\\n\")\n",
    "    print(\"Hi! I'm here to find you the perfect product. Just talk to me naturally!\\n\")\n",
    "\n",
    "    state = {\n",
    "        \"messages\": [],\n",
    "        \"collected_info\": {},\n",
    "        \"step\": \"asking\"\n",
    "    }\n",
    "\n",
    "    # Start the conversation\n",
    "    print(\"Advisor: Hey! Tell me about your skin or what you're looking for — I'm all ears!\\n\")\n",
    "    \n",
    "    config = {\"recursion_limit\": 30}\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in {\"exit\", \"quit\", \"bye\"}:\n",
    "            print(\"\\nAdvisor: Take care, come back anytime!\")\n",
    "            break\n",
    "\n",
    "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "\n",
    "        for output in app.stream(state, config):\n",
    "            for node, value in output.items():\n",
    "                if \"messages\" in value:\n",
    "                    msg = value[\"messages\"][-1]\n",
    "                    if isinstance(msg, AIMessage) and msg.content.strip():\n",
    "                        print(f\"\\nAdvisor: {msg.content}\\n\")\n",
    "\n",
    "        # Update state for next loop\n",
    "        state = app.invoke(state, config)  # just to keep state in sync (optional)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_advisor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb5fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Your Personal Skincare Advisor\n",
      "\n",
      "Hi! I'm here to find you the perfect product. Just talk to me naturally!\n",
      "\n",
      "Advisor: Hey! Tell me about your skin or what you're looking for — I'm all ears!\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oh, hello there, lovely! It sounds like you might have dry skin. Am I right? Knowing your skin type is a super start!\\n\\nTo help me find your perfect match, could you tell me a little bit about what your main skin concerns are? Are we looking to soothe, hydrate, brighten, or something else wonderful? 😊', 'extras': {'signature': 'CowDAdHtim9YaNy7eO/P+45FBdSGkEwi0GfN+b4orFgzgMqPHGBsqu517zccVdzAAq2LQA0JObREDOOAhC2KD71GbQW1wqjujpXYhJvMxTDvSzBQcVDbelhzKEaIeCbcoOXqFgHlHyDMJ0TlYsChXsag/lJgH4YEDVq9tZT1Uwi7+mwKUs7Q2zS5Ue9xqJ/V/SJyP3HZaWO0datiKAm/zyf6+lCPHv/Ky0KpPHUCPqoWaMpAdrXCIIx0Gph6pLtHBtNvFbjoBn80W/e8jHMHIUof9gZBkz622jag8g95Mdy3DCLkh/nWgk4hEdatT3YPP6Cw4iyGYGX+ObRc8XqvAkcGWINKFl326x7+JkKvLJ9MgYqZlGiItLc6P8b6Ao2vq4TumuXgoYG433iM/jrmJFJOciiQhzdw4Mt3NA6KaVQy35ZvQ9UDcuLbotCw4bd5nlrQhuvA5G/pEalzi7Hlf3jJXXMF2nSv5Js44FDrwrfgtYUsf5/o8E+z+9I3G6qcNPUutRf4Jdnho7GAYZ2O'}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Absolutely! So glad you're here. To help me narrow down the best products for you, could you tell me a little more about your main skin concerns? Are you hoping to tackle dryness, sensitivity, fine lines, or something else that's on your mind? Let's find that perfect product together! ✨\", 'extras': {'signature': 'CqsCAdHtim9ClEsIB44XrJPRww0jYCEkBItrrPQs4fQpnJdctsPRP86lyLy+g1UFslxwcDMBweNZYKcTsjEWukLPaFua45okd6rUfvuS6xvP1AMYgotNctwD8boqDMv2J73r5FjVwps6VL0DwdrJJQs1xV5CnEKDWbNfgXR0FG3UOLVs+Pbzx4kqqxF9gDfplO3GTCRBHSML44FFMX9S+mmiLyC3xtmZTbNNLGXGfkz3lMdNP6xt5nTbXpsoa+g5neocC64GcvDbZDgUL7PQTKqhGQyKLXIBDMZmIF0O+VrUd+qvn/+Y42S0IOVRRM5Lz2YpMnSr7x7xYYVZhW7zaySepFFeD32zs7tlK174xfizQxv4r/laDdwE2QZfkaHvJvQzF1WPnW3JlTrsexU='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Got it! Thanks for letting me know your main concern. That's super helpful!\\n\\nNow, to get an even clearer picture, what's your skin type like? Are you more on the oily side, dry, combination, or perhaps sensitive? Knowing this will help me find products that truly sing for your skin! 😊\", 'extras': {'signature': 'CoYCAdHtim8jkSW79HZdLZwtMEd2n9Ws6clTN5rKDcmtrogTvww5MckANU5KrR96DJcBTZDaS60ewO47YMbld1+g4W6WhW/UbOqoUO84FPU4q63u2Cr245Rw925RnQWEeEnLM9ZYcAmeyz7USY+I85pGD9HQDqy91LuHia86kxpwwwfE3rmckh7YHsy2MQ+nrgEIDTVt8RJAbf6tzeOtR2GA0bk6sh8YzdisAda+MERDqv72lRaL9QObl41k7oUgv5Om7py/YthP2MAOjQut9hcQbTE56jRQoA7Dsslnrb/ZC5tD4BobgoTMH7GmPNCQhVdb5aFWwg+Qd4cvxjZo2mI2oGwq9utz/g=='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oops, my apologies! It looks like I got a little ahead of myself. I'm still waiting to hear about your skin type! Knowing if you're oily, dry, combination, or sensitive will really help me narrow down the perfect products for you. Let me know when you're ready to share! 😊\", 'extras': {'signature': 'CvUBAdHtim+8BlMdbJ3O5UV8IKtxLpYwfDvt7IQuSZw0O7ql5/vA9Ycnhaw8Bv3wpzqnamqZmgbP+dW8GYv/5OiIjlejdfK0+9pekugG6xVw8mecHrTh19SmWmXEQmAbJs3/hS1/iQY2ZYmN2Uc1gQ9o8c1xxRRAN+YC+UOfFqGJutrpdNbhg3M05M9qcWhXKXLHxsQ1iz301oT/ZEO+bZ6AC+9EaHOVl9V2qVMnoktp3Hn7DC1clnIYt0Ak7XvIIzx2IC+jgKp9TfyrVIk+ESrL7dUMLcIsh2yPed3R1nCGAJoYAJEkqTrRBQEP1yoU5/hqD4ICs90='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oh, absolutely! So sorry about that little hiccup! To help me find your perfect match, could you tell me a little bit about your skin type? Are you more on the oily, dry, combination, or sensitive side? Knowing this will help me so much! ✨', 'extras': {'signature': 'CscCAdHtim/8OD71WVCE2jNlfljTuWuiq3v0z8O2fdAtvnKWskpedZhKLWasAnyFT1hXW3BdiWIvBa2z1lu/bj9e2eg/H/UGIyHvz3VRKyLMP5JjRquCWctvnTg+UMMIZcVsY3BPR48tnlaFRpH2riaPUro3enrK8kbZWJOWpUz2lpe+ZB824En3tF7ulnqH70qG5shEJpp5bp6iaRJUPzXHxUJ+qeKgYumE02Y29y6g12iRe/KvUFEJRNVbOLTtPBxOGLolr8LC0hJ4ufCzCKSLksbbdTS4fDMsMHjskNoC7k+Iz0OZiVvY0525iTD2eBWsFq3aDZtSCoIofSlJQ5Q/GfOSoqnl5+jf4QfjdrBwqWmimogUuDSi61QACvEALwFTCjaNcAadHVzrpIn07awzqr/echBTZVhPZy0Bk9NKkuxNxBgj/fyG'}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oops, my apologies! I got a little ahead of myself. I'm still waiting to hear about your skin type! Are you more on the oily, dry, combination, or sensitive side? Knowing this will help me narrow down the perfect products for you! 😊\", 'extras': {'signature': 'CvEBAdHtim/JuFvVsKiujzpd3F3998amu9WzD2Heyc4HOlgVT8BO8WkbQg99NwGwGQyhE7O5y4BhUItql9IeRs0j5yXyTRkawlElKqjsVA9wNH+FKBU0pjGMbiRj3YX+l5R1hTswsb8ZYySrsv2F9X9MVQ/U3UYYeAvDCS/+8kWwkaz5kg/uLtNNFlkwI1hq/ma0F8j7g1tjeUdmPwbqrhXU6tu4qU6H0Euu6/wYCdtx6g6hjNfn2QghdI1/fbJUQ8nDkWUUSTYQ+9tGKHgQY8gXwXmYrQLxLWu5ZcRW4JMAAGMMSGacfUY5Ut3sshoCiYNVww=='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oh, silly me! I'm so eager to help you find your perfect skincare match! To do that, I need to know a little more about your skin. What's your skin type? Are you oily, dry, combination, or sensitive? Let me know, and we'll get started! ✨\", 'extras': {'signature': 'CsoBAdHtim8aFDGFDbCAIk5iwNI+PNISCT3H2WZJ8SdJQKrQ6p6bHRW7o3yrT3HWmj4KiPjLvacyHHm12V6dk0b/sWnpRn8v5EvN06dCf8ocqRO4XL9/8mVU/I3kVkJbut3cKyreCfymb22WIDd5Ci3eo7pdgfal/fT800+1gYDlAimoz1NmQBet8YALZa6ydReTQf8HYQrQnY5qNj/XjedxTQcG0luz69YuXusEu8FZBkzzWReFwn6jrZW2WErIAO/ORHNBeDZIUQ1bfA=='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oops, my apologies! I got a little ahead of myself. To help you find that perfect product, I need to know a bit more about your lovely skin! What's your skin type? Are you leaning towards oily, dry, combination, or sensitive? Let me know, and we'll dive in! 😊\", 'extras': {'signature': 'CvgCAdHtim9FCbr3/9D+h6toEsHiEPMmFKDP6eyAikhEwdg07+V9rHbshbyo8R9Ih9KgLawsI7UdnMJwUvYIr39A5TL623oeNU7UXaIyBL8we348k+ta42usbPj7GixKwZLH51lyA2qZ6IkOPvJHRZe5fx2M3wTBKjk+YKNhKtNYN2z0dFaopgyzURema0K8Bklx/aUnoowjLLAi3eUj3udzGswolFwH1UHFhcE3mWcQl+Z8yHKopuk80qB8Mxj0ycSB3ii6gnw0yAkHX/C+7Uf4RkID4o6oGVLDKj64W2O5Iz4mCjIG7wm6MnHy7n95cfmswoJl85LA7C50xiKcD9dK/eiW4778mE4O9MJ4VRTRTzD3VIxPgyjNIvOVqkeMLA13JaK3cv4kgpfcTt6OuAkWLBPbpNLITDIf0W0ku3Y7xh9N+4tQiw6aa0Uj195F3n3Tqaq1z9Z9Oznc5KlPA/oM8ghNZ5KOCMosc0tsRsRtZ4mIHH/u2CoSYQ=='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oh, absolutely! My apologies, I must have gotten a little lost in thought there. 😊\\n\\nTo help me find the perfect product for you, could you tell me what your skin type is? Are we talking oily, dry, combination, or sensitive? Knowing this will help us narrow down the options!', 'extras': {'signature': 'CtoBAdHtim+1TI+vJAQIZYmxsKD/wt7n31NhrHY+p4yIyXKb8LFO6WzH/YxMumNySHt5IFzTi8qMorJjITVXnRvaYvTK4fhkXdJipKImBWg1uNdhqZxWVr5dYtay0JDUscFU88jrBr5/FyXHB1cpsfqxTaj8DlLO7wznKNH+Btf0LwnpAhlxQAWkiXgWemkXBCgPI4RfLjPxAWPrWS5DVcqBSjxy+99TfaprBqvJRXBFeKLONvvVF2tnpfuzJS9QOYcL61+UFQZ5RLQ+O7WMzrUKgawAt/ZHZid+g48='}}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 39.306586744s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 39\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oops, my apologies! I got a little ahead of myself. I'm still waiting to hear about your skin type! Knowing if you have oily, dry, combination, or sensitive skin will really help me find the best recommendations for you. What's your skin type, my dear? 😊\", 'extras': {'signature': 'CsECAdHtim8SRbMVFTYTiiyU4GfgeiYqgJ/qCKWHBPnf9Rk9t+JipkmpgLr8GKPE1fPj3A+e1KJbpyHcdVzvXMGxSzUi3mEZ8hkVRkpczmmeYgH+mfVAq/188w1jQ95Q0eocaHXMmhWdDkBQdgh55W3b+P20tb2Cny1WQbEI8cJV8oYV3K3Fbelvs95fTYK/Qelxb3M6DCwXWmXl80CUa2ilHoCM4EcSqp/BZLx3QGvtgS9KdhaDFM4vg5sAE6C8dVFNoMWxKGLokOCAdh5lh8khrM2TAWI21ksrE4J3ICOhQQBCNC7qA78Cr+crcbXBuA3WzwF3CZbnd8HF0cVQO5PvhHnOwXoOY5v/KJpkuwLf9WcdQWvLn06Lyo6lAsjqz5Vcvp/WSqwE6iquBzFNawE1OycDHoGnE1+nLTFdgmfLaJLT'}}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 37.173444965s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 37\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 33.057672342s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 24.928577188s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 8.783766232s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oh, I'm so sorry! I must have gotten a little lost in thought. My apologies! To help me find that perfect product for you, could you tell me a little bit about your skin type? Are we talking oily, dry, combination, or sensitive? Knowing this will be our first step to skincare bliss! ✨\", 'extras': {'signature': 'CqsCAdHtim+FWXDZAvEyJeWJjpuBlQEQLvS08DVdPA0SpygeSt7JqwxbiO8lBDdcwuzQeE9zDTPl0BPK8A/bXQsSKKZI1F4n1T0NL+oIxfFsMzCBVQLHwhbbBs0r0jvxW8+hfwb5iaqBggm+hsS3sR9QaacsO3YkUbRCTmbb2WzY3FwyTkihEnBBYktFSRZZ/++CG1Afr8eHHmJ3/ZZEccmlAXBlGL/3kDPIvosJagupX32lBGLrb+qz7kw0Ilaq1IfzfkJ8ak8U5nIFqklJ0/gFcL0prU2laO99x1Tj5pDW6CqtY+chyanWb5LcvTqghVo/Pu5g4CfvMWqImozz31m5DTdIXRPHDhYlwVC3V8qqqlrHfqx8OXZcvxdd9piDsO/3T2KGPW5GjC305Zo='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oops, my apologies for the little pause! I'm just so excited to help you find something amazing for your skin!\\n\\nTo get started, could you tell me a bit about your skin type? Knowing if it's oily, dry, combination, or sensitive will help me narrow down the best options for you! 😊\", 'extras': {'signature': 'CtoBAdHtim9MaAylua4dqoCGU2Jui8jTEdvDy3xpJUGhvyPA7sRm1dDatf4Rbh5g3ikf1XNPYku25KAnUOKbx51VMn5zoVMwLcNzm8JWMp2cuCMfqXi8z72C9nVRXs2QdiiQrTaJlqVfU0HfgwS/oPHxx5p2dmNLOl1WNNI+VxJV7cKhb9n2X5x9rhOIlb1kS+DyKh+IJBGT5AcgnOaBl6KEXTzFM/DmfsrJfSgW9ASYQh88OaG/7E/krBr2hE0xSYfy0xMS9iUOkcFcPmP3bnxYWDS0e6hKOD9lUWk='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Absolutely! I'm just waiting to hear about your skin type so we can dive into finding your perfect product! What's your skin type like? Oily, dry, combination, or sensitive? Let me know! ✨\", 'extras': {'signature': 'CtkBAdHtim+i9HBt5I8dnpnQG1nhLFlxZ7su4i5yOZ/m/K9eLDcLDPwTey8knm0q3TZGqMIp4LwK/klp1FUfLO72cddxdYYhavJ8MYqk0FWJfep/FAZujnvDEWNDlb0tgoEEcIsBOI2IAMGwqrZvMoqDGhV5lzkOzB2CJxg6E809QbDzWcI0t+2vaPVnkshjBfVDM8lj4x/kY4tG4eEw3v151AyuqTyfd/vDaDsYth7jn9bxrNaZaqbjLTI1FtQOOvkbjIFryvhLua/V+MW2hYmiZbj8e2Ga7HeILQ=='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Hehe, I'm ready when you are! To help me find that perfect product for you, could you tell me a little more about your skin? What's your skin type – oily, dry, combination, or sensitive? Let's get glowing! ✨\", 'extras': {'signature': 'CvIBAdHtim8egTPTI8ZYK4By73sBMw3Ab5BwQFQiCQ9d1T8azpIQ9S8FjeqztXX4iq10m9JGiTFycRN51GYPanjIa8StL/rwwEQXie1KKl01EkC/nuskxRZuv5/wDB7M2YoanbuPio6DHlkybw7wTkhxTPqmcuEZ5gklE2AUcHmasLmxoUDxwj4VXTTXk2jGlzJGuDT8iET0sOxh1TtG+cd2Bm8jPt2ObZBctRKVA6OuBooKziaISVW6VMkMYNFbH+LfyDPd5KSZLWcx+jOCqfMwDoNQMAE9qRdcqMjcGYCgqNAnni5Sc2GPYyNyGtmCXnS7LLM='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oops, my apologies! I got a little ahead of myself. To help me find that perfect product for you, could you tell me a little more about your skin? What's your skin type – oily, dry, combination, or sensitive? Let's get glowing! ✨\", 'extras': {'signature': 'Cq0CAdHtim9PkFn3Kdws9mN5XNliMod29QUHbZg+w1utJEfD58ByRssDPtjhztfgKo36LTX2BE+OYWwQ35I0hh+SXi1qAaeu2pPA+2Jfyeho6hEYvVPRmmw8CTg0rGoJuhLLsnW3Q5ffrWo/laEbrl/hu2ApMSVYgLIjBTXFiWqVynRgagQUIRL37+DXhYNchHi4pX119jREdyupGoyeGwmLrBJHITGShRuYsJqDVMtfyPMi10rdzRmeMoi6uLj+2NnvqwXgUgm5TxHtA4hgx41+wvLg2eR62Wqm6eirWe11edIH1R+b6KoFBYadmnOSxrUMgvRP0XodqTI6xEQi/12LMPodGyIAWClAmSys5sUNT8vT9xCHtcPj0c4hqO9i+t77cKonOyGEc21HKWUFcQ=='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Absolutely! I'm still here and ready to help you find your perfect match. To get started, could you tell me what your skin type is? Are we talking oily, dry, combination, or sensitive? Knowing this will help me narrow down the best options for you! 😊\", 'extras': {'signature': 'CvEBAdHtim8cxh4sB8ffyWOgI/OQq/5Geix1upIfDugqnjp59y0v0YBpMWzRYQK8tw3stvbIUXvfN6YcPOX3kj87ysCDtyrx/dARCAiaxdfDKszbF7s1lBDnuMP6yAKE3EHrTgUcpDgKCSZDU4y5dqMn6M85gddgyTejwKeJ58FPYbnQo81/L5/Iv1D4MvPkgVLnAtoBiH33iIpvFtIky6I0i/0DfnZAOW1Imf4UY+7fbso2lCV1ZQE7NJL0Actx1Y3ya2zCATpXh30eqIbFnW9a948D2r4hx7Q+PXO9c+lR+MYUwhkwcgunAatNL0vwuG0j9A=='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oops, my apologies! I got a little ahead of myself. I'm still waiting to hear about your skin type so we can get this skincare journey started! Are you oily, dry, combination, or sensitive? Let me know when you're ready! ✨\", 'extras': {'signature': 'CtkBAdHtim+MZ8V+lMPUoUbWP1Kixw4AV1K2MsmFJdftA3uNG0l2ZVxgTu+6Wc3ra9niL69Z6a2mspMwPnkPz1QkMj+of8zb1ijqTDf6ZCjSBg8t2iVvGXl4lm+Sb5upvtsk93M3mvs3aB60H/TxpHipre4VJFx/VV1R5IvBZp7DJF6oOSXqUCXc8kXf36z8SmKKY/lW+6Laeh0tEKsZeHxY5w+8xlGxT/DyKnf8JgxE1eN3LeYXXFgKZLjX9lelfW4ZKoCg0Sz9KXNiNoa9bDQne0xv6Fv1A5ErnA=='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oh, absolutely! So sorry about that little hiccup! To help me find your perfect match, could you tell me a bit about your skin type? Are you more on the oily, dry, combination, or sensitive side? 😊', 'extras': {'signature': 'CuoBAdHtim9lNdKhMPto5kaW/tMd2LtPd2ez1c6ipPcuvPrBaxxfRvF8Ej9nC1903dd9R7vrZPufo5FjyXrvx2DLktCdNRu9d4GAh7BY3+V3R240NMzEbnBBWIqZi9rdl08Ml+Swd+dH8wxAaVZjrh9DR8DHHhGeEvkMKhiKv2fHH0+9suK+fm+gyhCzyCQxq9zPZ0EL+uIuD4GAtCQg+ZgIDLz+e7wHezXFXM3317D8oAiyPcyI3bNk2sMFHVw06bXLIbBETHPt2iLGXUi6T4a4g5jfmF9K0Xju1FFsC6DWL6N0TlWUWeGTemqn'}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oops, my apologies! I got a little ahead of myself. To help me find your perfect match, could you tell me a bit about your skin type? Are you more on the oily, dry, combination, or sensitive side? 😊', 'extras': {'signature': 'CvABAdHtim835xHrCzwDHJSnl1iGBERXzbmDwcIuS9ZVa/TMwr5zbEa6Yn/TtcFLQrhYlaPxQCOutKyRaO0DB/zs36DTMGVM7pBCRdduBhkGaBkLswEPvH4coAhynJJi2fsziAznOxdakjTDJb5sWZqljNFmCIGt/5GfN84zd81MwGqq0+YyQ/UbLPrTTpFwxcHRaSK0z33G9lP15Zkut9d/kVo6d1U93vTlCbjbcg2w0hm3x/lGlFViwbNvyoOMTqdg2QQvGRVbvwRPP7WMHQgYj7VeGqVoDsD1bHkgIQIsU2K6eYV6H/bldF23mwZJzpML'}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oh, absolutely! So sorry about that little hiccup! To help me find your perfect match, could you tell me a bit about your skin type? Are you more on the oily, dry, combination, or sensitive side? 😊', 'extras': {'signature': 'CtkBAdHtim8GxGu132suXU82CaaTqKAWbbhe3QB/vNQrzXW1Uy7McTnIqcOJA2w+4wxrQnXHJtrgTNAGX1C5oiIE4UevPKHSP7x5hiRnuTmy+4xC3679g37323YOQbeHqyXrraFFM/bGMAOnHHsOfEnSTUsyn2sBNi3XVUnzlpaVpO2UpHXQ9lh0tKIWjb8JwSViHSfez8LpfrmON2N8mhN6Oxc6S+o6JykvxqS8ncAZl6LYKwzDwLxV3qk6kXf2jChk/uMisQsYUiYfGIVgvm0kUCH/SKbVog4MrQ=='}}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 23.417772384s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oops, my apologies! I got a little ahead of myself. To help me find your perfect match, could you tell me a bit about your skin type? Are you more on the oily, dry, combination, or sensitive side? 😊', 'extras': {'signature': 'CvABAdHtim9ONFETf9CuYGErf/0DK/qSoRX7io4H9dXR6XnAm9LHX0vpr2yBsGg1JhASK251SdOtDzJVwR3dTllbkftXiW4Z2bUZZwHAu9GiXIJQd5NxhN5/6OSHeQdE1KmvH94vVb13sSIkNfpprwrV0GsvJBfOtQWDoJb9v0S5uo5XiZ9qFlijeJ2KzxwvPmQ4aSRsJDBqQ0tLjbiBx0FUEOOw68oo5zWDEMJ3QhZ70bVhkSpVJa3TUhqc7PsxYKES0lwiTOlFuAeFjT+RztCgKyrr/q+DD0sf7EX+bzOqGw5jNBpbU417EHcxSNZCNNY+'}}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 21.258296893s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 17.007549068s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 8.840303325s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oh, absolutely! So sorry about that little hiccup. To help me find your perfect match, could you tell me a bit about your skin type? Are you more on the oily, dry, combination, or sensitive side? 😊', 'extras': {'signature': 'CtoBAdHtim8fUIULXntxZPrmFhNO2IbUp4xHYwICsPGzh+4lusOt8a8IfR6Y1VRIXRJPRqPe8vxINlnc3iLiuQ+4Uohu15ivP/WKUa2Vi1CswCT2Nk9OhIS3OSariqmMa1CjzUqVRA5dL+wdBVY3xlIveTObCdUeKwpMgl66xElbGT0uW/Dp9bR9KAbYl+qA9NJi9NJxzEZNndq2/jUCGvxCo9BoNOUMwDREu0YTkMg2ZXdY8QMeCws0CzsUk1Zho9qptZFES24Bd2QtuRakRbGwOfhDOCHnxQ9weyI='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Oops, my apologies! I got a little ahead of myself. I was just wondering, what's your skin type? Knowing if you're oily, dry, combination, or sensitive will help me narrow down the best options for you! ✨\", 'extras': {'signature': 'CvABAdHtim+5rAMjtEUx4fvOHE5UOXzjtddQCyehF+4UKwHG56+vnuNEUIBTmyll48sNZkFargzF3FjM0y2+p1csacKxzfFiDLyJT+HNZhYVIwKFhhNP1LcdZyPOQps0BREb6Bm+5AQ8ZKy1t8aCMYyejafA+/B1sfgXkP7PQOFD2EO6DJKApnj62vURj3kfz2NatwfSO+Fe6Ily/cz+SqqJhtlA1RwSAtKSfxL4+u9vRGSkrbs8qqPvYWKb+/tw6ZwyZh5+3v1wQbvTLUM/y7tcsXcN2meUXnwRM7EYJqKJw8Q4KFNxFIbGjsqGmH5G07Ic'}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Absolutely! To help me find your perfect match, could you tell me a little about your skin type? Are you on the oilier side, feeling a bit dry, a combination of both, or perhaps a bit sensitive? Knowing this will help me so much! 😊', 'extras': {'signature': 'CuYBAdHtim8tIV6HBywObJuz3O9dTX/O4iy8RUSpP1aC3kByxilXC4KZmzpvYhZP6+xhthg2jsqrzln2VO7AocWAYO3iwg0nBOgDJA/TOEsK2x1hr01JOFhjGYVPN7G9hpS3T86lwXcYR0lYwbA5H8DM00EkVjnsmbc6Hq/9fCEQALxoyQeYKZ1PTtIBbJvwGGVwSQVePM7YtAB6iIBJko38kymQ5KvKTFlHPTpcN+N8xxzp2OoaWXIa1clcnQXiUablG3Og2LNy4raoiUrcRPowQAGLRVsvojI2JwfkCJbVClEvENKQsBI='}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Got it! Thanks for sharing. Now that I know a bit about your skin type, what's your main skin concern or goal? Are you looking to tackle breakouts, soothe redness, hydrate deeply, or something else? Let me know what's on your mind! ✨\", 'extras': {'signature': 'CpECAdHtim98HCxIJkaoBJokzoy7zal6gFv4GTFzq0LSwfyib/t+dW/fURGFb5v5DrgAE30fBMKXYgx4w5jJiO+U0+W4+k839Rjg/xs6p1l6s0iD9NxmlNnabq6pAIkjzpIkCiYAdBW1m/K2jKIbie8eSfEzPJv7w/vMI3uk1En6ssyPKTk17DBrtrzdTcx08rIdg34tqOEP1Jp5UavutGJkcryoSGFzcvCQJx1pztpUf46EmTpq6ejavVrUQ7M4j+qomGSEpKLJseqw1yWm5qpcVLT0XLolK6Wcxn1kH21l+GwfMmxaynqNTD45jr8HNe574nXySAetAB/hUi8LGwf55JkoTQDqDBDvPUng8LQ+i7Sc'}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': \"Absolutely! I'm excited to help you find your perfect match. To get started, could you tell me a little about your skin type? Are you on the oilier side, dry, combination, or perhaps sensitive? Knowing this will help me narrow down the best options for you! 😊\", 'extras': {'signature': 'CqMCAdHtim8yAJXxpEnTJ6wtH0CJGh9Qm03+9k1rcRb8z5TKI7j0fin7NOg7PWfGZHtM+cKZqjvzFcwFGMD3acysU1fzJnPZxRkWeL+NTihf3VKSqXZQY5spuAu6sP9avi+rcMqM02voNINRbTHxA79C5sDWi+cKgU6SSWEPGyrCG0bb8+8ECeairn8ZEaXH5L6nIHUJtiYG+31eIydHyRkEd7XTWq0DTYB74jUvFL/ctYBitZi/1qyMGHsKF38+eQZmHLs8t1GCywwjuIF9l0KvPwQ2gbOYHcEPQMY0VBldeevgTDzjTi34hFXfqlirZKoweAjKMa71sKv6G31BHdb7t5Bp1aaDxAWHGkVDLb85sQoaj2J8OuC405N5RjvgcnYMML1P'}}]\n",
      "\n",
      "\n",
      "Advisor: [{'type': 'text', 'text': 'Oops, my apologies! I got a little ahead of myself. To help me find the perfect product for you, could you tell me a bit about your skin type? Are you more on the oily side, dry, combination, or perhaps sensitive? Knowing this will help me narrow down the best options! 😊', 'extras': {'signature': 'CoMCAdHtim9/56EvVdlOhsyBkS46BYKqtwAuiVzEZWCr8VpGgnPmWALtj6/aDucFQNJKNnvGUumINXLgHy9CrEDvlJr+PjtTT+Q9Hp7iiJghTV3LvZ7uZCNxv35WDcil95Lgw2vUYTWrWI1Y0/a1mNAxVtJngX9raffuM/91pCdg0tasb9XE3Wn5dtNA8/ikBpYHNtMOTb9n9CYlf4KnLcya50siv9mr7iVDDFpQsUypEbBCmqiVk3t2wiyMUrZd9UR5WDIbfKR5DObcYP2gq+XWI/6fDrngmjis+8NdFmPvmRVW5Kem6T0hkAPLGjSz+5qUWuCLgj7kx6DiutUolHlu/erkHA=='}}]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 242\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Ignore if already at end\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 242\u001b[0m     run_advisor()\n",
      "Cell \u001b[1;32mIn[3], line 223\u001b[0m, in \u001b[0;36mrun_advisor\u001b[1;34m()\u001b[0m\n\u001b[0;32m    220\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(HumanMessage(content\u001b[38;5;241m=\u001b[39muser_input))\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Stream and safely print only AI responses\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(state, config):\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m value:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:2633\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2632\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2634\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2635\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2636\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2637\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2638\u001b[0m ):\n\u001b[0;32m   2639\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[0;32m   2641\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[0;32m   2642\u001b[0m     )\n\u001b[0;32m   2643\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     run_with_retry(\n\u001b[0;32m    168\u001b[0m         t,\n\u001b[0;32m    169\u001b[0m         retry_policy,\n\u001b[0;32m    170\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    171\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[0;32m    172\u001b[0m                 _call,\n\u001b[0;32m    173\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[0;32m    174\u001b[0m                 retry_policy\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[0;32m    175\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[0;32m    176\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39mschedule_task,\n\u001b[0;32m    177\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m    178\u001b[0m             ),\n\u001b[0;32m    179\u001b[0m         },\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 656\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 400\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m, in \u001b[0;36minterviewer\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m messages \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], HumanMessage):\n\u001b[0;32m    102\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease continue.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 104\u001b[0m response \u001b[38;5;241m=\u001b[39m llm_with_tools\u001b[38;5;241m.\u001b[39minvoke(messages)\n\u001b[0;32m    105\u001b[0m tool_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m    106\u001b[0m new_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearching\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_calls \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masking\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5534\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5527\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5529\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5532\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5533\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5535\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5536\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5537\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5538\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2119\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   2116\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.code_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 2119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:385\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[0;32m    380\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    383\u001b[0m         cast(\n\u001b[0;32m    384\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 385\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m                 [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m                 callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m                 tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m                 metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m                 run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m                 run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m             )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    395\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[0;32m    396\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1104\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1103\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:914\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    913\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 914\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    915\u001b[0m                 m,\n\u001b[0;32m    916\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    917\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    918\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    919\u001b[0m             )\n\u001b[0;32m    920\u001b[0m         )\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    922\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1208\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1209\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1210\u001b[0m     )\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2270\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   2269\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries\n\u001b[1;32m-> 2270\u001b[0m response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   2271\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2273\u001b[0m     generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m   2274\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   2275\u001b[0m )\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:258\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    255\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    256\u001b[0m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service\n\u001b[0;32m    257\u001b[0m }\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m action(retry_state)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: rs\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:232\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mmessage:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:869\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m    870\u001b[0m     request,\n\u001b[0;32m    871\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m    872\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    873\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    874\u001b[0m )\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_call(\n\u001b[0;32m    278\u001b[0m         request,\n\u001b[0;32m    279\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    280\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    281\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    282\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mwait_for_ready,\n\u001b[0;32m    283\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\grpc\\_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[0;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[0;32m    331\u001b[0m )\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:78\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[0;32m     68\u001b[0m     }\n\u001b[0;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         },\n\u001b[0;32m     77\u001b[0m     )\n\u001b[1;32m---> 78\u001b[0m response \u001b[38;5;241m=\u001b[39m continuation(client_call_details, request)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     new_method,\n\u001b[0;32m    308\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     new_compression,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thunk(new_method)\u001b[38;5;241m.\u001b[39mwith_call(\n\u001b[0;32m    316\u001b[0m         request,\n\u001b[0;32m    317\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mnew_timeout,\n\u001b[0;32m    318\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mnew_metadata,\n\u001b[0;32m    319\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mnew_credentials,\n\u001b[0;32m    320\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mnew_wait_for_ready,\n\u001b[0;32m    321\u001b[0m         compression\u001b[38;5;241m=\u001b[39mnew_compression,\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:1195\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1190\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[0;32m   1192\u001b[0m     (\n\u001b[0;32m   1193\u001b[0m         state,\n\u001b[0;32m   1194\u001b[0m         call,\n\u001b[1;32m-> 1195\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1196\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1197\u001b[0m     )\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence, Literal\n",
    "\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ===============================\n",
    "# LLM & Embeddings\n",
    "# ===============================\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # Correct and available as of Nov 2025\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Load & Index Products\n",
    "# ===============================\n",
    "products_file = \"products_descriptions.txt\"\n",
    "if not os.path.exists(products_file):\n",
    "    raise FileNotFoundError(f\"Products file not found: {products_file}\")\n",
    "\n",
    "loader = TextLoader(products_file, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_products\",\n",
    "    collection_name=\"products\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# ===============================\n",
    "# Tool\n",
    "# ===============================\n",
    "@tool\n",
    "def search_products(query: str) -> str:\n",
    "    \"\"\"Search the product database.\"\"\"\n",
    "    results = retriever.invoke(query)\n",
    "    if not results:\n",
    "        return \"No products found matching your criteria.\"\n",
    "    return \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "tools = [search_products]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# ===============================\n",
    "# State\n",
    "# ===============================\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], \"Conversation history\"]\n",
    "    collected_info: dict\n",
    "    step: Literal[\"asking\", \"searching\", \"recommending\"]\n",
    "\n",
    "# ===============================\n",
    "# System Prompt\n",
    "# ===============================\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a warm, expert skincare advisor. Your job is to help the customer find the PERFECT product.\n",
    "\n",
    "Be friendly, caring, and a little playful. Speak naturally.\n",
    "\n",
    "Ask ONE smart question at a time when needed.\n",
    "\n",
    "When you have enough info (at least skin type + main concern), you may call the search_products tool.\n",
    "\n",
    "If the user says \"done\", \"end\", \"recommend\", or \"that's all\" → call the tool immediately.\n",
    "\n",
    "Already known:\n",
    "{collected_info}\n",
    "\n",
    "Respond ONLY with natural conversation.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# Nodes\n",
    "# ===============================\n",
    "def interviewer(state: AgentState) -> dict:\n",
    "    collected = state.get(\"collected_info\", {})\n",
    "    info_str = \"\\n\".join([f\"- {k}: {v}\" for k, v in collected.items()]) if collected else \"Nothing yet\"\n",
    "    formatted = SYSTEM_PROMPT.format(collected_info=info_str)\n",
    "\n",
    "    history = state[\"messages\"][-20:]\n",
    "    messages = [SystemMessage(content=formatted)] + history\n",
    "\n",
    "    # Gemini needs last message to be Human\n",
    "    if not messages or not isinstance(messages[-1], HumanMessage):\n",
    "        messages.append(HumanMessage(content=\"Please continue.\"))\n",
    "\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    tool_calls = getattr(response, \"tool_calls\", [])\n",
    "    new_step = \"searching\" if tool_calls else \"asking\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"collected_info\": collected,\n",
    "        \"step\": new_step\n",
    "    }\n",
    "\n",
    "def extract_info(state: AgentState) -> dict:\n",
    "    last_user_msg = next((m for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), None)\n",
    "    if not last_user_msg:\n",
    "        return {\"collected_info\": state[\"collected_info\"]}\n",
    "\n",
    "    prompt = f'''\n",
    "Extract skincare preferences as JSON (only confident keys):\n",
    "\n",
    "Possible keys: skin_type, concerns, preferred_ingredients, avoided_ingredients, texture_preference, budget, size, routine_step\n",
    "\n",
    "User said: \"{last_user_msg.content}\"\n",
    "\n",
    "Return valid JSON only.\n",
    "'''\n",
    "\n",
    "    try:\n",
    "        result = llm.invoke(prompt)\n",
    "        data = json.loads(result.content)\n",
    "        current = state.get(\"collected_info\", {}).copy()\n",
    "        current.update({k.strip(): v.strip() for k, v in data.items() if v})\n",
    "    except:\n",
    "        current = state.get(\"collected_info\", {})\n",
    "\n",
    "    return {\"collected_info\": current}\n",
    "\n",
    "def tool_node(state: AgentState) -> dict:\n",
    "    results = []\n",
    "    last_ai = state[\"messages\"][-1]\n",
    "    for tc in getattr(last_ai, \"tool_calls\", []):\n",
    "        result = search_products.invoke(tc[\"args\"])\n",
    "        results.append(ToolMessage(tool_call_id=tc[\"id\"], name=tc[\"name\"], content=result))\n",
    "    \n",
    "    return {\"messages\": results, \"step\": \"recommending\"}\n",
    "\n",
    "def recommender(state: AgentState) -> dict:\n",
    "    collected = state.get(\"collected_info\", {})\n",
    "    search_result = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a luxury skincare consultant giving the final recommendation.\n",
    "\n",
    "Customer profile:\n",
    "{json.dumps(collected, indent=2)}\n",
    "\n",
    "Search results:\n",
    "{search_result}\n",
    "\n",
    "Recommend 1–3 best products. Explain why each is perfect.\n",
    "Be warm, enthusiastic, use light emojis.\n",
    "\n",
    "End with: \"Which one feels right for you, or want more options?\"\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=prompt),\n",
    "        HumanMessage(content=\"Give me the final recommendations now.\")\n",
    "    ])\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "# ===============================\n",
    "# Graph\n",
    "# ===============================\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"extract_info\", extract_info)\n",
    "workflow.add_node(\"interviewer\", interviewer)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"recommender\", recommender)\n",
    "\n",
    "workflow.set_entry_point(\"extract_info\")\n",
    "workflow.add_edge(\"extract_info\", \"interviewer\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"interviewer\",\n",
    "    lambda s: s[\"step\"],\n",
    "    {\"asking\": \"extract_info\", \"searching\": \"tools\"}\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"recommender\")\n",
    "workflow.add_edge(\"recommender\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# ===============================\n",
    "# Run — FIXED PRINTING LOGIC\n",
    "# ===============================\n",
    "def run_advisor():\n",
    "    print(\"Welcome to Your Personal Skincare Advisor\\n\")\n",
    "    print(\"Hi! I'm here to find you the perfect product. Just talk to me naturally!\\n\")\n",
    "\n",
    "    state = {\n",
    "        \"messages\": [],\n",
    "        \"collected_info\": {},\n",
    "        \"step\": \"asking\"\n",
    "    }\n",
    "\n",
    "    print(\"Advisor: Hey! Tell me about your skin or what you're looking for — I'm all ears!\\n\")\n",
    "\n",
    "    config = {\"recursion_limit\": 200}\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in {\"exit\", \"quit\", \"bye\", \"end\"}:\n",
    "            print(\"\\nAdvisor: Take care! Come back anytime\")\n",
    "            break\n",
    "\n",
    "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "\n",
    "        # Stream and safely print only AI responses\n",
    "        for output in app.stream(state, config):\n",
    "            for node_name, value in output.items():\n",
    "                if \"messages\" in value:\n",
    "                    new_msgs = value[\"messages\"]\n",
    "                    # Handle both single message and list of messages\n",
    "                    if isinstance(new_msgs, list):\n",
    "                        for msg in new_msgs:\n",
    "                            if isinstance(msg, AIMessage) and msg.content and str(msg.content).strip():\n",
    "                                print(f\"\\nAdvisor: {msg.content}\\n\")\n",
    "                    elif isinstance(new_msgs, AIMessage) and new_msgs.content and str(new_msgs.content).strip():\n",
    "                        print(f\"\\nAdvisor: {new_msgs.content}\\n\")\n",
    "\n",
    "        # Optional: keep state fully updated\n",
    "        try:\n",
    "            state = app.invoke(state, config)\n",
    "        except:\n",
    "            pass  # Ignore if already at end\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_advisor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cabf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Your Personal Skincare Advisor\n",
      "Hi love! I'm here to find your perfect match\n",
      "\n",
      "Advisor: Hey gorgeous! Tell me about your skin — is it dry, oily, combo, or sensitive?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence, Literal\n",
    "\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ===============================\n",
    "# LLM — Now using Grok-4.1-fast via OpenRouter\n",
    "# ===============================\n",
    "llm = ChatOpenAI(\n",
    "    model=\"x-ai/grok-4.1-fast:free\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # Put your OpenRouter key in .env\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Product Database\n",
    "# ===============================\n",
    "products_file = \"products_descriptions.txt\"\n",
    "if not os.path.exists(products_file):\n",
    "    raise FileNotFoundError(\"Missing products_descriptions.txt\")\n",
    "\n",
    "docs = TextLoader(products_file, encoding=\"utf-8\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100).split_documents(docs)\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_products\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# ===============================\n",
    "# Tool\n",
    "# ===============================\n",
    "@tool\n",
    "def search_products(query: str) -> str:\n",
    "    \"\"\"Search for skincare products matching the user's needs.\"\"\"\n",
    "    results = retriever.invoke(query)\n",
    "    return \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results]) if results else \"No matching products found.\"\n",
    "\n",
    "tools = [search_products]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# ===============================\n",
    "# State\n",
    "# ===============================\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], \"Conversation history\"]\n",
    "    collected_info: dict\n",
    "    next_step: Literal[\"collecting\", \"searching\", \"recommending\"]\n",
    "\n",
    "# ===============================\n",
    "# Nodes\n",
    "# ===============================\n",
    "def interviewer(state: AgentState) -> dict:\n",
    "    info = state.get(\"collected_info\", {})\n",
    "    prompt = f\"\"\"You are a warm, caring, playful skincare bestie.\n",
    "\n",
    "What you already know:\n",
    "{json.dumps(info, indent=2)}\n",
    "\n",
    "Rules:\n",
    "- Ask ONE short, natural question at a time\n",
    "- Never repeat or apologize\n",
    "- Only call search_products when you have both skin_type AND concerns\n",
    "- If user says \"done\", \"recommend\", \"ok\", \"that's all\" → search now\n",
    "- Be fun, kind, and human\n",
    "\n",
    "Respond naturally. No lists.\"\"\"\n",
    "\n",
    "    messages = [SystemMessage(content=prompt)] + state[\"messages\"][-10:]\n",
    "\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    tool_calls = getattr(response, \"tool_calls\", [])\n",
    "\n",
    "    # Grok calls tool when ready → we trust it\n",
    "    next_step = \"searching\" if tool_calls else \"collecting\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"collected_info\": info,\n",
    "        \"next_step\": next_step\n",
    "    }\n",
    "\n",
    "def extract_info(state: AgentState) -> dict:\n",
    "    last_user = next((m for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), None)\n",
    "    if not last_user:\n",
    "        return {\"collected_info\": state[\"collected_info\"]}\n",
    "\n",
    "    prompt = f'''Extract only as valid JSON. Keys: skin_type, concerns, budget, preferred_ingredients, avoided_ingredients\n",
    "\n",
    "User said: \"{last_user.content}\"\n",
    "\n",
    "Return JSON only.'''\n",
    "\n",
    "    try:\n",
    "        result = llm.invoke(prompt)\n",
    "        data = json.loads(result.content)\n",
    "        current = state.get(\"collected_info\", {}).copy()\n",
    "        current.update({k.strip(): v.strip() for k, v in data.items() if v})\n",
    "    except:\n",
    "        current = state.get(\"collected_info\", {})\n",
    "\n",
    "    return {\"collected_info\": current}\n",
    "\n",
    "def tool_node(state: AgentState) -> dict:\n",
    "    results = []\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    for tc in getattr(last_msg, \"tool_calls\", []):\n",
    "        result = search_products.invoke(tc[\"args\"])\n",
    "        results.append(ToolMessage(tool_call_id=tc[\"id\"], name=tc[\"name\"], content=result))\n",
    "    return {\"messages\": results, \"next_step\": \"recommending\"}\n",
    "\n",
    "def recommender(state: AgentState) -> dict:\n",
    "    profile = state.get(\"collected_info\", {})\n",
    "    products = state[\"messages\"][-1].content if state[\"messages\"] else \"No products found.\"\n",
    "\n",
    "    prompt = f\"\"\"You're giving the final recommendation!\n",
    "\n",
    "Customer wants:\n",
    "{json.dumps(profile, indent=2)}\n",
    "\n",
    "Products found:\n",
    "{products}\n",
    "\n",
    "Recommend 1–3 best ones. Be excited, warm, use emojis.\n",
    "End with: \"Which one are you feeling most?\"\n",
    "\n",
    "Be natural and fun.\"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=prompt),\n",
    "        HumanMessage(content=\"Give your final recommendation now.\")\n",
    "    ])\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "# ===============================\n",
    "# Graph — Perfect flow\n",
    "# ===============================\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"extract_info\", extract_info)\n",
    "workflow.add_node(\"interviewer\", interviewer)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"recommender\", recommender)\n",
    "\n",
    "workflow.set_entry_point(\"extract_info\")\n",
    "workflow.add_edge(\"extract_info\", \"interviewer\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"interviewer\",\n",
    "    lambda s: s[\"next_step\"],\n",
    "    {\"collecting\": \"extract_info\", \"searching\": \"tools\"}\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"recommender\")\n",
    "workflow.add_edge(\"recommender\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# ===============================\n",
    "# Run — Beautiful conversation\n",
    "# ===============================\n",
    "def run_advisor():\n",
    "    print(\"Welcome to Your Personal Skincare Advisor\")\n",
    "    print(\"Hi love! I'm here to find your perfect match\\n\")\n",
    "\n",
    "    state = {\n",
    "        \"messages\": [],\n",
    "        \"collected_info\": {},\n",
    "        \"next_step\": \"collecting\"\n",
    "    }\n",
    "\n",
    "    print(\"Advisor: Hey gorgeous! Tell me about your skin — is it dry, oily, combo, or sensitive?\\n\")\n",
    "\n",
    "    config = {\"recursion_limit\": 30}\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in {\"exit\", \"quit\", \"bye\"}:\n",
    "            print(\"\\nAdvisor: Take care, beautiful! Come back anytime\")\n",
    "            break\n",
    "\n",
    "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "\n",
    "        # Run one full cycle\n",
    "        try:\n",
    "            for _ in app.stream(state, config):\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            print(f\"Oops: {e}\")\n",
    "            break\n",
    "\n",
    "        # Print only the clean AI response\n",
    "        last_msg = state[\"messages\"][-1]\n",
    "        if isinstance(last_msg, AIMessage):\n",
    "            text = str(last_msg.content).strip()\n",
    "            if text and not text.startswith(\"{\") and not text.startswith(\"[\"):\n",
    "                print(f\"\\nAdvisor: {text}\\n\")\n",
    "\n",
    "        # Update state\n",
    "        try:\n",
    "            state = app.invoke(state, config)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_advisor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9947412",
   "metadata": {},
   "source": [
    "# Simple 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def8f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.11-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Downloading psycopg2_binary-2.9.11-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 8.7 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ba94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AI SKINCARE SELLER AGENT READY ===\n",
      "\n",
      "Agent: What is your skin type?\n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'MEMORY UPDATE:\\nskin_type = \"oily\"\\nWhat is your main skin concern?', 'extras': {'signature': 'CqUCAdHtim/A4GZKLTL+ec15Fs0ivIvfT1b/318yFmBT+y9xm4uSOTj4PRPl4yT0WhrE+nKOEPv5i9oTeYaKgC7G7zFj27X0JDwpKAOwCiXNxveVTqytcWQ90b1vpIJ/ipMovqy9T2XyL1SHg+1GKe/Lj0YcSGWHt/hJDBkjUCkuB3k8CXexOIqmStwummOiDDtSdjCqfHEoSdbdCdFvr7dE/qmUvktCQefatR/BX+HW2OLUFNTY6atf73nIS7Qjgr/Z0+/YwQVm9Kubs8haHaHq/hRnV9mE3+n508hyqmoq1X3orD4PU/PISdYTsxGWdtUZ5Mb5/vIUVXiH02iSlNFWSF0z6abc3MR7qS4ySF4MKrleAh8KOAXryJ6IE4AOUBkXJRs9Un4='}}]\n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'MEMORY UPDATE:\\nconcern = \"extras\"\\nWhat is your budget?', 'extras': {'signature': 'CoMIAdHtim/AMHuCVROe8qj6w68NJUnJ+/NMN4qqgnkOX5bLAVQ20JMcPBTOdkOCDv+UA3fsPpKjtQb3lVcotdWDhA6kZyVBF5XUJpCBpw7EYVI7GT95DRD/65bQTYAum2yT6wSTBQmk4pq3xO835ArCdPQGl3os68DUX72JlP0cXe1VDMHbSRc1OdEqK8ECUg14GgdvC8AOpJp1ijJWtyHCGVRBU2iIVTDfJ5z8sGZmKyeMbjU+y551ceNNdLwh6prSYpWL1G+ui3pPY54f3mET2TzfygZQk6wNIB5zzVxteKLWE3sEVz4hFRZv4P6ddI8bOHwImytJkUQxEYotFV/Wg/ErPmgIGJBP6pDEvcaO2FplXBXUFRA/wVa9OQWn5Qmvd8sRliLwVMsfRHuB0dPN3qmW+KPB9D3zh01ow9vNWeRjjm7KCiPKGptnY6xH7TeOCYiI1fW4Ann52PlOcN/0c2QWBJmBHDBQxMz7WBN+/yRtA1bVwv943VvGzoL/WRLKBJQpZHGJ9UIW5o8Rj+Q+m4jZiuwuNVD6TDiFYIU5TapPNKpBYsIdRXFy5EEPPid3JjwgMMtk0/5UP7jDMMvxedvBr9xBqZfde1yVvzruPf0erWchafBTvei4HuCtCe8mH5hJFpSGcpZwK8S5rr12aG47AU2Drs0HMN0V1ez7smfuOPxlrxcZUg0w59rZYcNHYYXDd4O4XUFuSh77MVtTOZ8QOfXuI4yeaDtCek50loIXo6Uv0EvJ8SuVLFQReSMjCJygX90GLNGDe5zlp/Z/M3Vxm7J/qOXiF4fWaECsRvZnt9B+ZN9RjYOGltJceUYz1fh19akqPzYzTMMKhCoXaJ1ERkaGwUhw6ElfawxQiJC2v1pYwnxmdyIQskQ9+Buba/fd6zOD2H9No0l0LhiDzsAxMT6F2PD73t29qpsLc7JSGZw6hLBYJ+geTeqO3IiY5fbmqULsyptzNzapVc4HH9KtpZg+KiaP+wXxiSvz0t3K0hoRzklGVB7iuR4vvI1U6noHR4EvIYXT3+zqm93OdQPgT1d0HEofIvgqx0B+E1U0vpESz68k1gY1QjJmaot/Vkd7PWivv1tOzQwIUnxR6Qr0UNBA3DwL6wEeMtwgPlOIXrpGrAJ+WOmOyKqtPIN9OPO+zbD9BhpZiyUIdbn4ihfnaZskzBnxY0zaMuiG6BRD08Mb9hB30F7XPyQrekdVPDU2GY8OTtmcv/FC4H0ZhKAnpS0IqlygG+wf/zcAC7FOL5CZb8VgcSmm8y9+CQSwFXF+OvhrsR2BBakQZiuOD8fF3rBkwcg2QY9jlZsnx1KRF976cDrXQI0F93O3JAcwhq1O8axOt2r2Hj92nGXJufqKAg=='}}]\n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'MEMORY UPDATE:\\nbudget = \"140\"\\nDo you prefer gel, cream, or serum?', 'extras': {'signature': 'CscBAdHtim//gwfnR5NJrUc9VoJEnF3gUKxtHXqE1ibv6wcqkxMkjqQCg5YAoM6IabvYMA+kH0M5astB0vbczMkNW0evtnk/qzrP1jdJF6uTlxlwJfSTxjpuCmcELFu5Bw1Uum6z6Kkacs0VwK1STIfMCRbVIARp2TS4hiHmTUCUtFK0GkVcAECC8/AkGV844PVDyul3iUrlIbVF3zNu5h9+xQqIJ+4wNB9bR5gZ+CBIg9aprLH0ruPFOg5Ij7J7qnhnhOne+gZDXA=='}}]\n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'Do you prefer gel, cream, or serum?', 'extras': {'signature': 'CvoIAdHtim9VT+huEz+01NHGvnZcaOVq20HNcCvUvLBYh9VBdoLWla5QAQ8Pc76tNnlCCIhc5r/PyAgm+/8+5OfQ5QLNHpF8lMmzJW3WO2kv6PKx9NXpX00VxJiYOC9G8VlljBap3JNH2K2PAbsV6a7KG8fBNiVZJ/j2QW8YLgZXrjB7Q91v/c9MeM5tjh1LEYxiCnO8kTK+6kE+fduEQ6fTLlbyrnRvH95Iqo+c2sL77ItprvtO7tUJ4xgPvDQkiWFLwkYGuNFHai6F2kHRZAWPJPZtEXhDgEm4o/k64A64HXigum5RfLOyZWfY4Xj1M/++OympGKNju6/yG/XEWiRfT1oE/+nlDolYnt2Wg1/gF7k3OiT24fWrNam80PhOfFqp+WzmUD7JYDlL3Kql1+EB1v7NEkHubDv0VpE8t0ffk5K1qlKyrDD0QpTG0woz/ReZfpj5qyPxTs6d8cnudTYhZkQFbleq9JzEQazc0dWmqmxt/sMT62K+yB6IEjUhgmepSekhWpTHVRcYjzahwZ1XHLwpBjg4l7KghEwKy4iR8A/QOemUwTrjmmkyR3JYVI+JD2KlA9YW42Zcbi7tjZSCmkG5+RZjAz0RkqV55lVuqLrfJUb3FBGoKq2lXHSu5hRtw3Tq4Kxff/fFgkiSfGYyn8y13hsPdHvdtV+u6ixrMfJdN7WSi/d58YkGQEViLsWweVAh6o8NMWCR/MDGFvgoxXiDpCbsrEb1SI5SIOPybitpqDHQo/MnoErGIwz6V4w9V48lerASqOIElAKJvbaNh6gVysJIfn4jo4TObBbe7jHZUn+mw5ynta9mwce73F0za0ao6L7zckCgrgW9aFjiA7LL1tOp1Wg6gLObfAe1eD8kaQuOUVZcLAqSjKFsOtmVcsg926m4rHWe0ZnNd2bcmACEsCQHj1qp8cA+ClJIau7m2Ti5Mib4gU7nVwQaGit1VjrXKgt14Gq2B5xKuvnVoo9DvH/l4dm3o50hZ23NoG5y2l84ZbJDl3FZQS2qr/oGNCoClspKYA3fhir1melY2jrSY/1quSG6zPqk6DInkOOXl1d7UKOb59wwJM+App9RUTt5CoQeLN2G6+ehp+ZRtIND5+QuU/SvP8Y0wFT4FXRz/XNaLOzuAuzyTjmvuVhJbtAPAZHkqJRRujzpLVGu4dKtASDJcd9mdUC0A1Hm5lQ2LcTSEfa+U9KTEDjDH6weNueU1gQuLPKcCtjO5N8HZtY7G2D0zKkgjpwzAekHuVz2qLYNISSME28Nazl2xR2ocDtyQGxnR0D+0c0KV+nEExU5BxBeJppc6p/p71cNJLFRat/z+ynHThlk8blzxW/w5k4YpnScCZY/Pmw+bsOOyanoKvSFuTBjvZlmMUQj6N/BKyBCTOpPdkxXH5dWoalgDgfcNmUtUuLVI/RaeQoH5SStCcIXGF4FZ0ZyqXDoe6wO8TH14mtHxFE499pckUEfpGt9vqScEQX82tXUB0FF9tV5gJwsf4uTEilsPAjYq+TzHy7pNNAMhvL8'}}]\n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'I couldn\\'t find a serum that perfectly matches your criteria within your budget. However, I found \"Product2 - Mattifying Gel Cream\" which is suitable for oily and combination skin, helps control shine, minimizes pores, and reduces acne breakouts. It contains Niacinamide, Zinc PCA, and Salicylic Acid.\\n\\nPlease note that this is a gel cream, not a serum, and its price is 160 MAD, which is slightly above your stated budget of 140.', 'extras': {'signature': 'CoUGAdHtim9N0dqk1ttH5nRtQ8UQ/8bqrsLcGaSpQ68FXRp/NtFWSYzYq6Rf2oNP9Vog1KCiID3dip/O8PyTzjGnLWGILmJUJVlUfZnpf9FFi/1N407EdXQXPUHzGbYxKejy3GDCoADo2deDlG7/pAvrLLJykCf31G1AFuFhrdtqpoPuaNlydQE6QUmfX0PTP6PwKLfO6SNVRsy0fx9oxwCS9ysNnw0HoPSiIpii0oSi7gWnwzMF+j7lydLhsbogjnES6o2NJhNQkwnIEu2iAjhYSNJEGzVvboYZt2acjI1XfLdqjoUU2roQfKSPGQSYvcfqc0xc2k1uWfEkVeYTyWZ6r6PhjFi41VEHeGNATjVPnP6xCe/lE7cuT5rNjKfP90vMbUdo+QIgd3tAotaAtD2wFoxGnfGdtfTs4st3E8tyFYoiRdpEa7CsWPwVLFzCVKQBopgNKiH9ckgSWqcgQO+6eNI9k8eumobRE9+XivO6awhRXtrLOjN1/NizkHc44VtdUUDHlSvksFUXr1k7CHADDCMDneI02mnFbtcSUSP5wbrkItIAJ74Hx56zo8gxkP8d0ZWetpiKd5uLtueYn8u+zkiBhiObX5DuexQvyafxLqCwhMuyNDwu5wfq0i32c+BgN9M2AEum8vJQOn+OXzRPTNrMkTUiMJvj3nx1GNuSkgwGNUUwm41lPo2n9VMXHuKIFBUp3gWcwnN3nooTi4nbHv8xEYkbx4i1VV0oTWvB4m4g7FiYAFJ/j5kyttyYmE+wmddhmtmts69OJRHFnQjGO4UtH8thNhqxE5rIUieAmfebNgwtEZy1JQAPPjSKQ6xPQp1R3Hv1nyjfWCmxatqI7ldDWN0ZeyYbU6pNZE969WRRAS4vpqyr/ArWneOpr+9OL6lHSe6bg99bpHfomrfJTbiOUzdJLT8Q6IRrUM5QeYvtWiZ1rwFRdwSDAcAJ39O5Dv/okOgMUIvX3W/kO+pirO9e3GICBd7WjSNUY7qi+DKdxIYdSVLW1WVtCuxIRy2+ZMEZ52Y='}}]\n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'I can only search product descriptions using the `product_search_tool` that is available to me. I do not have access to a \"neaon database\" or any other external databases.', 'extras': {'signature': 'CtUCAdHtim+t0eVNWCQfh28k7FQlJizZYHaq/VniJ9YMC10jG6jt8ng96BbGpnNT2G4KkiTFWz4bgl9pNNIQoBHxNGPhoKjegs5EiADt7Ldvzh8U/PicYGflum9Zr6GSAh9bN7muusYjY2HYg1bO1hdCjVywdjI0AvHfq1QZV6E+7ilj8TUvkNyiocdqNkjeCAH9eyp3NsvCIIwdq8UkFPCxuKB30YAKf3xZnwlqqiUqlzl+Zeg6RAFOXZ6JgZcXDxEZRr8e4ZW1JU5G1i9DCQD1HDDCQa855uvzQnCYBbGhU+DboBPBdRUcr1+ADeufZZIavpWlmIvg7ydUXHazQHYaLJ8Nu/swjRoEdEnSETkdB01K/EKw30iDc3DlC54iUvKl93eFcy9+km1w8iiLFtAFusOKoFjmiVO5nhw8zBY6CMPOkiQ5ZGKCaK6OmIVRHdAI4qm57BA='}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2512: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent: \n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'I only have access to the `product_search_tool` to search product descriptions. I do not have access to specific named databases like \"neaon\" or any other external databases.', 'extras': {'signature': 'CuwBAdHtim/0CAyacmNVD7vJF5h9jYC5O6t1g2P3PoCqiu6AfPDVGUxeOZJiyu4hpIa34GJV00I1jaXJPIjB4rsJF0j9JLpfOnFi3od81JtkoegrzU06xb+/oEh7HhO9y8xJSefkZSYkZd8IaXisjAv+vrOquHo9r2EQ57FSB3XkuqB9ml62s7Ywx5BzWt5XS5X7txKHYAtQRSvLDohWKFIJPimlwqHhli7KiQY0NIBh4k9jRXIQT5/ilgH1c0Rt0o+foWQGGsCKG/pj/b8L+C7X5z0sUpL08pnr4XmZOC0gQsfMus75LfQGcpmStws='}}]\n",
      "\n",
      "Agent: \n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'I only have access to the `product_search_tool` to search product descriptions. I do not have access to specific named databases like \"neaon\" or any other external databases.', 'extras': {'signature': 'CuwBAdHtim/tl+i7MOMpBd7HoQxODLZv5HSAE1z+3nnH1nkkN91koE6YjZhv28myx43AsJ1336X21wA7iDPzTM+D1yjeh8GbIXhraXSm0cw8MEkJcyfhM10mKm9c71XP+rTTjULthCapth8uZaYvjGXfOfHxWaoBm/dlZm3cdMYl5Ktu4i6eCWa3X9i3rALFlcEC+9kOgrrh/FX9uDJdDOjxBIU5CYvilc2h5TnwuoLEZZfWGHmCbh+QOnzHXQ99BlXLGNlwMN8MSvvXlNR/tFd2UiBCy3FboZLuJDNKcNTUwOyf4QAv9w+kwhB0pPw='}}]\n",
      "\n",
      "Agent: \n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'I only have access to the `product_search_tool` to search product descriptions. I do not have access to specific named databases like \"neaon\" or any other external databases.', 'extras': {'signature': 'CuwBAdHtim+4IV3C5QJoAkU+nEmj1AgJ9t/Oh5FVzkKfeoBcy0DJPZnkny5AsqZpmFRX3A8BqXEOHlH+FDkGdukRYJX45WkaOg5YQH1tgut4a81xNO8ATJyedcsd8i3zo4d6Q8A7KxhXCbCwchepZTJkPZCMqF28mLsOh2smqptsccZZma0djYkwum2abW2sPDI619KfulWPUrJVQKA3GmKLhfVN7EPK0Ln46DDv0xwrZhrC22/UuXwbVAxTWiQiWttU3oigpqx7rz8l4Yvtnl7yJBCx5LtOqMOe4g5B6cln6vc5wZ6rHX5v5fwfU3Q='}}]\n",
      "\n",
      "Agent: \n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'I only have access to the `product_search_tool` to search product descriptions. I do not have access to specific named databases like \"neaon\" or any other external databases.', 'extras': {'signature': 'CuwBAdHtim9hhAGBfjMj+kOgHEGHENlTdRVDdgu4zR+ow8gs6EV26hNdNEzMXufoJ4Bli8h8nOyYlv6BLcJgk0lTz1EGEZ7UvRnSiXvC/uZ5X5VihYDJNAj/ZI9n1oSJuPI6ASCQQquXH8rWxPNjwD7MykNHUHeG46AvVogHbM+tJUQL/yVr/ieNJAT/dtlf4gDiOHZMDxCJD6CaJqUU3X6gQdDKURnG0AYSR3sYhX4VjFmk+TO8bGbxzpqgLt1A4PXBGchu8LNLR89U/EZfFBHbC8+Ar5NaeQT9bWZjw+h4PvmhDPThn37ZXIj6Pgk='}}]\n",
      "\n",
      "Agent: \n",
      "\n",
      "Agent: [{'type': 'text', 'text': 'I only have access to the `product_search_tool` to search product descriptions. I do not have access to specific named databases like \"neaon\" or any other external databases.', 'extras': {'signature': 'CuwBAdHtim+Qjaz5tr0RzBjCxMr/6+5mSqFpYj1LFlFTUU13FkpF8qTsDF8iCmCAi7EQDtt5/OjDTBdX1dniTFZm2Ht6Y1JRi9MDI53NtzW6CTmDocHjyH1KuiRYfQ4/+OtUDP8wqH2E+Jcb5kVtsgVnXa1vKCk3R91lIkEAa3K4n58FIQpoCeGsno1aNMd4efXNifLFeQQ1wzK3CEniyCabW7uk36SeoA38YnXqumongv9jS/F0ZT9MlKJwC3E2aKuJaPDTAHfdNcoykfJh/c2t1Ys3TgawfUcLTICm6iS7lmuFHkzDIbH1+Vb7OXc='}}]\n",
      "\n",
      "Agent: I only have access to the `product_search_tool` to search product descriptions. I do not have access to specific named databases like \"neaon\" or any other external databases.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# Neon DB connection info from environment variables\n",
    "NEON_URL = os.getenv(\"NEON_URL\")  # e.g., 'postgresql://user:pass@host:port/db'\n",
    "\n",
    "@tool\n",
    "def get_product_from_db(product_name: str) -> str:\n",
    "    \"\"\"Retrieve product details from Neon database.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(NEON_URL)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "            SELECT name, description, skin_type, key_ingredients, benefits, usage, precautions, price, size\n",
    "            FROM products\n",
    "            WHERE name ILIKE %s\n",
    "        \"\"\", (f\"%{product_name}%\",))\n",
    "        \n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        if row:\n",
    "            return f\"\"\"\n",
    "Product Name: {row[0]}\n",
    "Description: {row[1]}\n",
    "Skin Type: {row[2]}\n",
    "Key Ingredients: {row[3]}\n",
    "Benefits: {row[4]}\n",
    "Usage: {row[5]}\n",
    "Precautions: {row[6]}\n",
    "Price: {row[7]}\n",
    "Size: {row[8]}\n",
    "\"\"\"\n",
    "        else:\n",
    "            return \"Product not found in database.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Database error: {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage, \n",
    "    SystemMessage, \n",
    "    HumanMessage, \n",
    "    ToolMessage,\n",
    "    AIMessage\n",
    ")\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Google LLM / Embeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Text Splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector DB\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ENV\n",
    "# -----------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# LOAD PRODUCTS\n",
    "# -----------------------------------------------------------\n",
    "file_path = \"products_descriptions.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\"products_descriptions.txt NOT FOUND!\")\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "documents = [Document(page_content=file_content)]\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# SPLIT CHUNKS\n",
    "# -----------------------------------------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# VECTORSTORE\n",
    "# -----------------------------------------------------------\n",
    "persist_directory = r\"C:\\Users\\HP\\Desktop\\M2SI_2\\ML\\Ai-Agent-Products\"\n",
    "collection_name = \"products\"\n",
    "\n",
    "os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs_split,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# TOOL\n",
    "# -----------------------------------------------------------\n",
    "@tool\n",
    "def product_search_tool(query: str) -> str:\n",
    "    \"\"\"Searches product descriptions.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    if not docs:\n",
    "        return \"No matching product found.\"\n",
    "\n",
    "    results = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        results.append(f\"Match {i+1}:\\n{doc.page_content}\")\n",
    "\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "\n",
    "tools = [product_search_tool]\n",
    "tools_dict = {t.name: t for t in tools}\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# LLM\n",
    "# -----------------------------------------------------------\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ").bind_tools(tools)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# AGENT STATE\n",
    "# -----------------------------------------------------------\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    msg = state[\"messages\"][-1]\n",
    "    return hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ONE-BY-ONE QUESTION SYSTEM PROMPT\n",
    "# -----------------------------------------------------------\n",
    "system_prompt = \"\"\"\n",
    "You are a skincare seller agent. \n",
    "You MUST follow a strict step-by-step dialog flow.\n",
    "\n",
    "You maintain an internal memory in the conversation:\n",
    "- skin_type\n",
    "- concern\n",
    "- budget\n",
    "- texture\n",
    "\n",
    "RULES:\n",
    "1. Only ask ONE question at a time.\n",
    "2. DO NOT ask the next question until the user answers.\n",
    "3. After getting an answer, store it in memory like this:\n",
    "\n",
    "MEMORY UPDATE:\n",
    "skin_type = \"value\"\n",
    "or\n",
    "concern = \"value\"\n",
    "etc.\n",
    "\n",
    "4. Dialogue flow:\n",
    "\n",
    "IF skin_type not filled:\n",
    "    Ask: \"What is your skin type?\"\n",
    "    STOP.\n",
    "\n",
    "ELIF concern not filled:\n",
    "    Ask: \"What is your main skin concern?\"\n",
    "    STOP.\n",
    "\n",
    "ELIF budget not filled:\n",
    "    Ask: \"What is your budget?\"\n",
    "    STOP.\n",
    "\n",
    "ELIF texture not filled:\n",
    "    Ask: \"Do you prefer gel, cream, or serum?\"\n",
    "    STOP.\n",
    "\n",
    "ELSE:\n",
    "    - Build a search query combining all memory fields.\n",
    "    - Use product_search_tool\n",
    "    - Recommend the best product.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# LLM NODE\n",
    "# -----------------------------------------------------------\n",
    "def call_llm(state: AgentState) -> AgentState:\n",
    "    messages = [SystemMessage(content=system_prompt)] + list(state[\"messages\"])\n",
    "    \n",
    "    # If there is no human or AI message yet, add a dummy one\n",
    "    if not any(isinstance(m, (HumanMessage, AIMessage, ToolMessage)) for m in messages):\n",
    "        messages.append(HumanMessage(content=\"Hello\"))\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# TOOL NODE\n",
    "# -----------------------------------------------------------\n",
    "def take_action(state: AgentState) -> AgentState:\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    msgs = []\n",
    "\n",
    "    for t in tool_calls:\n",
    "        tool_name = t[\"name\"]\n",
    "        query = t[\"args\"].get(\"query\", \"\")\n",
    "        result = tools_dict[tool_name].invoke(query)\n",
    "\n",
    "        msgs.append(\n",
    "            ToolMessage(\n",
    "                tool_call_id=t[\"id\"],\n",
    "                name=tool_name,\n",
    "                content=result\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": msgs}\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# GRAPH\n",
    "# -----------------------------------------------------------\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"llm\", call_llm)\n",
    "graph.add_node(\"tool_agent\", take_action)\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"llm\", should_continue,\n",
    "    {True: \"tool_agent\", False: END}\n",
    ")\n",
    "\n",
    "graph.add_edge(\"tool_agent\", \"llm\")\n",
    "graph.set_entry_point(\"llm\")\n",
    "\n",
    "rag_agent = graph.compile()\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# RUN\n",
    "# -----------------------------------------------------------\n",
    "conversation_state = {\"messages\": []}\n",
    "\n",
    "def running_agent():\n",
    "    global conversation_state\n",
    "    print(\"\\n=== AI SKINCARE SELLER AGENT READY ===\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nClient: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        \n",
    "        new_state = {\n",
    "            \"messages\": conversation_state[\"messages\"] + [\n",
    "                HumanMessage(content=user_input)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        conversation_state = rag_agent.invoke(new_state)\n",
    "        print(\"\\nAgent:\", conversation_state[\"messages\"][-1].content)\n",
    "\n",
    "running_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000277F279D1F0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0bbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5be5b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1847d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StateGraph' object has no attribute 'to_dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use the original StateGraph, NOT the compiled one\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dot_code \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mto_dot()\n\u001b[0;32m      5\u001b[0m Source(dot_code)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StateGraph' object has no attribute 'to_dot'"
     ]
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "# Use the original StateGraph, NOT the compiled one\n",
    "dot_code = graph.to_dot()\n",
    "Source(dot_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4cc592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating beautiful graph visualization...\n",
      "Could not generate PNG: name 'rag_agent' is not defined\n",
      "But here's the ASCII version:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rag_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# This creates and saves a perfect PNG\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     rag_agent\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mdraw_mermaid_png(output_file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskincare_agent_graph.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUCCESS! Graph saved as: skincare_agent_graph.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rag_agent' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not generate PNG: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the ASCII version:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(rag_agent\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mdraw_ascii())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rag_agent' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# VISUALIZE YOUR AGENT GRAPH — ADD THIS AT THE VERY END\n",
    "# -----------------------------------------------------------\n",
    "print(\"\\nGenerating beautiful graph visualization...\")\n",
    "\n",
    "try:\n",
    "    # This creates and saves a perfect PNG\n",
    "    rag_agent.get_graph().draw_mermaid_png(output_file_path=\"skincare_agent_graph.png\")\n",
    "    print(\"SUCCESS! Graph saved as: skincare_agent_graph.png\")\n",
    "    \n",
    "    # Auto-open the image (works on Windows/macOS/Linux)\n",
    "    import webbrowser, pathlib\n",
    "    webbrowser.open(str(pathlib.Path(\"skincare_agent_graph.png\").resolve()))\n",
    "    \n",
    "    # Also show clean ASCII version in terminal\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"YOUR SKINCARE AGENT GRAPH (ASCII):\")\n",
    "    print(\"=\"*70)\n",
    "    print(rag_agent.get_graph().draw_ascii())\n",
    "    print(\"=\"*70)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate PNG: {e}\")\n",
    "    print(\"But here's the ASCII version:\")\n",
    "    print(rag_agent.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9dc9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d161702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
